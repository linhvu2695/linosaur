---
date: 2025-01-06 02:26:40
layout: post
title: LoRA - how to make elephants dance
subtitle: The most effective way to fine-tune huge models to specific tasks
description: Low-Rank Adaptation (LoRA) method is a fine-tuning method introduced by a team of Microsoft researchers in 2021. Since then, it has become a very popular approach to fine-tuning LLMs, Diffusion, and other types of AI models.
image: https://jk-illustration.com/wp-content/uploads/2023/05/ss_101.jpg
optimized_image: https://jk-illustration.com/wp-content/uploads/2023/05/ss_101.jpg
category: Algorithm
tags:
  - training
  - ai
author: linhvu2695
paginate: true
---
In May 2020, OpenAI introduced ChatGPT-3, shocking the world with its science fictional capabilities, able of making responses to questions at human-like level. Microsoft partnered up with OpenAI with the mission of training models that aimed at more specific tasks, such as coding or translation. However, full fine-tuning a model like GPT-3 is prohibitively expensive: one checkpoint is 1TB large and take minutes to load. This is when an efficient fine-tuning algorithm has to be invented. One year later on 2021, <a href="https://arxiv.org/abs/2106.09685">Low-Rank Adaptation (LoRA)</a> was introduced, allows efficient fine-tuning with high-speed training and low memory requirements, while preserve the performance.

# How does it work?
This technique is generalization of fine-tuning by asking two questions:
1. Do we need to fine-tune all the parameters?
2. How expressive should the matrix updates be?

<img src="https://res.cloudinary.com/dptj6j9y9/image/upload/v1739116070/Screenshot_2025-02-09_at_10.47.34_PM_ittiwq.png">
Traditional full fine-tuning will be the point on top-right corner. Any point in this box would be a valid LoRA configuration. The below figure illustares clearly how these 2 apporaches differ:
<img src="https://res.cloudinary.com/dptj6j9y9/image/upload/v1739116244/Screenshot_2025-02-09_at_10.50.27_PM_abjuu4.png">
Any vector transformation process involve a matrix of d x d parameters. However if we transform the original vector to a smaller one of size r << d, then transform it back to dimension d, the total parameters count would be greatly reduced. This whole concept is called **matrix decomposition**: any matrices can be decomposed into 2 lower rank matrices. 

In the context of LoRA, we’ll refer to these two smaller, decomposed matrices as the “change matrices,” because they track the changes we want to make to the model’s weights.
<img src="https://velog.velcdn.com/images/d4r6j/post/2f0d087e-6873-4b7f-bf06-346c917b924c/image.png">
The A & B matrices are learned during training, and multiplied together to create the change matrix during inference. In this way, we reduce the trainable parameters, from 175B - 1TB / checkpoint for GPT-3, to just 4.7M - 25MB / checkpoint using LoRA!

# What is the right rank?
The memory reduction of LoRA comes with a cost: we now have to train a huge matrix using 2 much smaller matrices, which can be much less precise if the rank is too low. And if it's too high, than the benefits of LoRA can be gone. It turns out that, there is very little statistical difference between ranks of 8 and 256. So if you're rank is 8 or above, it simply may not matter.

## QLoRA
The QLoRA team created a special datatype called a NormalFloat that allows a normal distribution of weights to be compressed from 16-bit floats into 4-bits and then restored back at the end with minimal loss in accuracy. If your model uses 16-bit floats and you can store that as 4-bit floats, you've effectively reduced your memory footprint by 4x. That's huge. QLoRA should be considered an holistic upgrade to LoRA that allows high-quality fine-tuning of even larger models on smaller GPUs than ever before.


## Alpha
When the weight changes are added back into the original model weights, they are multiplied by a scaling factor that’s calculated as alpha divided by rank. By adjusting α, LoRA can balance between preserving the knowledge of the base pre-trained model and incorporating task-specific knowledge.

`W_adapted = W + α⋅A⋅B`

# How to use LoRA?

As long as a model uses matrix multiplication - which in fact all models do - the 2 questions above can be applied, and LoRA can be utilized. 
<img src="https://res.cloudinary.com/dptj6j9y9/image/upload/v1739116985/Screenshot_2025-02-09_at_11.02.49_PM_ywq1lc.png">
The nature of LoRA module is additive. The adapted models can form a tree, with each non-root node can be a LoRA module on top of the sum of its ancestors. The model rank can be large near the root and smaller near the leaves to accomodate different dataset sizes. Model switching now become tree traversal, and we never have to load the base model more than once.

LoRA also enhances the capabilities of Stable Diffusion by providing specialized styles and characteristics. These models adjust the base model’s weights to generate images in specific styles or themes.
<img src="https://static1.squarespace.com/static/6213c340453c3f502425776e/62f2452bc121595f4d87c713/655c90f932dda45e84c36b21/1728899860281/SVD+blog+cover+image.jpg?format=1500w">












