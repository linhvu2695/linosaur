---
date: 2025-01-10 12:26:40
layout: post
title: RAG - the open-book master
subtitle: LLMs can now go through the books for you and get the answer you want
description: Low-Rank Adaptation (LoRA) method is a fine-tuning method introduced by a team of Microsoft researchers in 2021. Since then, it has become a very popular approach to fine-tuning LLMs, Diffusion, and other types of AI models.
image: https://chatgen.ai/wp-content/uploads/2023/12/DALL%C2%B7E-2023-12-02-14.56.08-Create-a-minimalistic-image-that-features-the-acronym-RAG-in-large-bold-letters-at-the-center.-Surround-the-letters-with-simplistic-flat-icons-of-d-1200x686.png
optimized_image: https://chatgen.ai/wp-content/uploads/2023/12/DALL%C2%B7E-2023-12-02-14.56.08-Create-a-minimalistic-image-that-features-the-acronym-RAG-in-large-bold-letters-at-the-center.-Surround-the-letters-with-simplistic-flat-icons-of-d-1200x686.png
category: Algorithm
tags:
  - architecture
author: linhvu2695
paginate: true
---
In the ever-expanding universe of artificial intelligence, large language models (LLMs) have taken center stage. These sophisticated AI systems power everything from chatbots to content creation tools, offering unprecedented capabilities in understanding and generating human-like text. However, like any pioneering technology, they are not without their limitations. Asnwers can be outdated, or in certain occasions, LLMs might struggle with finding the answer and will jsut go ahead and - **hallucinate** an answer to please users!
<img src="https://towardsdatascience.com/wp-content/uploads/2023/11/14PuXxZJra3Ki1GE1aKqvIQ.png">
*ChatGPT - at the time when GPT Search had not been a feature yet*

Traditionally, neural networks are adapted to domain-specific or proprietary information by fine-tuning the model. Although this technique is effective, it is also compute-intensive, expensive, and requires technical expertise, making it less agile to adapt to evolving information.

In 2020, <a href="https://arxiv.org/abs/2005.11401">Patrick Lewis et al.</a> introduced the term **Retrieval Augmented Generation** (RAG) in a groundbreaking paper, positioning it as a key method for improving the accuracy and reliability of generative AI models. This mechanism allows the model to pull in the most relevant and up-to-date information from a vast database, essentially ‘augmenting’ the model’s knowledge base in real time. RAG specifically addresses two critical challenges: sourcing accurate information and ensuring that the knowledge is current.

# Concepts
In simple terms, RAG is to LLMs what an open-book exam is to humans. In an open-book exam, students are allowed to bring reference materials, such as textbooks or notes, which they can use to look up relevant information to answer a question. The idea behind an open-book exam is that the test focuses on the students’ reasoning skills rather than their ability to memorize specific information.

The vanilla RAG workflow is illustrated below:
<img src="https://towardsdatascience.com/wp-content/uploads/2023/11/1kSkeaXRvRzbJ9SrFZaMoOg.png">

Imagine we invent a new board game, but is highly complex with a lengthy manual. Users might want a chatbot that can answer their questions as they play with their friends.
1. **Parsing and Chunking**: first step, our manual needs to be chunked into smaller paragraphs, where relevant information might be nested within various sub-sections. The challenge lies in effectively parsing the structure to accurately chunk the data, ensuring that context is maintained even when similarities are not apparent. Decisions need to be made regarding the granularity of chunking—whether it should be by paragraph, line, or including metadata. Additionally, there might be a need for sliding window chunks to preserve the context from preceding text.
<img src="https://towardsdatascience.com/wp-content/uploads/2024/10/1TlSNAqNGGxk8C2NocaNfdQ-1536x864.jpeg">
*Depending on the different levels of chunking, your results may vary*

2. **Creating Embeddings**: an embedding is a vector that holds the meaning of a chunk of text. At this stage, the method of creating embeddings is crucial as it impacts the subsequent retrieval quality.
3. **Retrieval**: this is a critical step where the goal is to retrieve the most relevant embeddings in response to a user query. For a question *"How many points will I get if I buy this lot?"* should leads to the retrieval of articles about *Values of Slots* in your game manual. The query can even be analyzed (using the LLM itself) to extract its intention. 
<img src="https://towardsdatascience.com/wp-content/uploads/2024/10/18lCf7gpGTFmEE2yZANQEzg-1536x864.jpeg">
*Why we need to extract the intention before retrieving documents*
For getting the related documents, KNN search is the most frequently utilized approach. But sometimes, keyword-based retrieval, such as TF-IDF or BM25, can also be used. When you ask a query mentioning a very specific niche model or a serial number, KNN will fetch documents that kind of resemble your query, but may fail to exactly match the number. A common thing these days is to retrieve using both keyword and embedding based methods, and combine them, giving us the best of both worlds.
4. **Synthesis**: the final step involves synthesizing the retrieved information into a coherent response. The way in which the prompt is constructed for the language model can significantly affect the quality and relevance of the response. We can structure our prompt as follows:
`Give me answer to the question: "How many points will I get if I buy this lot? Your answer should based on these documents: {relevant_documents}. If the answer cannot be derived from the documents, please say 'I don't know'. Do not imagine answers."`

This approach has been shown to be highly effective for improving the accuracy and reliability of LLMs. Companies are using this method to build up chatbot for their own documentation and product manuals. On the other hand, this would also raise the question about information confidentiality, and how to ensure data safety when we the context to the 3rd party LLMs.

# Evaluation
## RAG Triad of Metrics
The RAG Triad involves three tests: context relevance, groundedness, and answer relevance.

1. **Context Relevance**: Ensuring the retrieved context is pertinent to the user query, utilizing LLMs for context relevance scoring.
2. **Groundedness**: Separating the response into statements and verifying each against the retrieved context.
3. **Answer Relevance**: Checking if the response aptly addresses the original question.
<img src="https://chatgen.ai/wp-content/uploads/2023/12/ezgif-2-dec5b52644.jpeg">

## RAGAs
RAGAs (RAG Assessment) is a framework that aids in component-level evaluation of the RAG pipeline. It requires the user query (question), the RAG pipeline’s output (answer), the retrieved contexts, and ground truth answers.
* **Context Precision and Recall**: Assessing the relevance and completeness of the retrieved context.
* **Faithfulness**: Measuring the factual accuracy of the generated answer.
* **Answer Relevancy**: Evaluating the pertinence of the generated answer to the question.
* **End-to-End Metrics**: Including answer semantic similarity and answer correctness.
All metrics are scaled from 0 to 1, with higher values indicating better performance.