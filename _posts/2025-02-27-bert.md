---
date: 2025-02-27 01:36:57
layout: post
title: BERT - always listening, always understanding
subtitle: The first digital brain that can actually comprehend human language
description: The introduction of Transformer architecture has been a Tunguska in the technology world. And scientists did not take long to start utilizing it to create monsters of artificial intelligence - starting with BERT.
image: https://www.codemotion.com/magazine/wp-content/uploads/2020/05/bert-google.png
optimized_image: https://www.codemotion.com/magazine/wp-content/uploads/2020/05/bert-google.png
category: model
tags:
  - nlp
  - ai
  - architecture
author: linhvu2695
paginate: true
---
Language has historically been difficult for computers to ‚Äòunderstand‚Äô. Sure, computers can collect, store, and read text inputs but they lack basic language context. So, along came Natural Language Processing (NLP): the field of artificial intelligence aiming for computers to read, analyze, interpret and derive meaning from text and spoken words. This practice combines linguistics, statistics, and Machine Learning to assist computers in ‚Äòunderstanding‚Äô human language. Individual NLP tasks have traditionally been solved by individual models created for each specific task. That is, until‚Äî BERT!

BERT, short for Bidirectional Encoder Representations from Transformers, is a Machine Learning (ML) model for natural language processing. It was developed in 2018 by researchers at Google AI Language and serves as a swiss army knife solution to 11+ of the most common language tasks, such as sentiment analysis and named entity recognition, making it the jack of all NLP trades. You can access the Github for BERT <a href="https://github.com/google-research/bert">here</a>

# What can BERT do?
First thing to note about BERT - it is an **Encoder-only** model! It means this model is trained to **understand lanaguage**, different from Decoder-only models such as GPTs that are trained to generate language. Imagine it as a know-it-all student - it can learn all the words, phrases, grammar and have a decent conception of the surrounding world. It can read and understand all novels, can listen and understand all conversations. It can decide whether the speaker is angry or joyful (**sentiment analysis**), can quickly summarize long legal contracts (**text summarization**), and predict your text when writing an email (**text prediction**). 
<img src="https://huggingface.co/blog/assets/52_bert_101/BERT-example.png">
*Pre-BERT Google surfaced information about getting a prescription filled. Post-BERT Google understands that ‚Äúfor someone‚Äù relates to picking up a prescription for someone else and the search results now help to answer that.*

# How does BERT work?

## Large amounts of training data
To make BERT understand different aspects of the world, a massive dataset of 3.3 Billion words has contributed to its knowledge. BERT was specifically trained on Wikipedia (~2.5B words) and Google‚Äôs BooksCorpus (~800M words). These large informational datasets contributed to BERT‚Äôs deep knowledge not only of the English language but also of our world! üöÄ Training on a dataset this large takes a long time. BERT‚Äôs training was made possible thanks to the novel Transformer architecture and sped up by using TPUs (Tensor Processing Units - Google‚Äôs custom circuit built specifically for large ML models). ‚Äî *64 TPUs trained BERT over the course of 4 days*.

Demand for smaller BERT models is increasing in order to use BERT within smaller computational environments (like cell phones and personal computers). 23 smaller BERT models were released in March 2020. <a href="https://huggingface.co/docs/transformers/model_doc/distilbert">DistilBERT</a> offers a lighter version of BERT; runs 60% faster while maintaining over 95% of BERT‚Äôs performance.

## Masked Language Model
MLM enables/enforces bidirectional learning from text by masking (hiding) a word in a sentence and forcing BERT to bidirectionally use the words on either side of the covered word to predict the masked word. This had never been done before! Imagine your friend calls you while traveling on the streets of Ho Chi Minh City and their service begins to cut out. The last thing you hear before the call drops is:

Friend: ‚ÄúOops! I'm crossing the street and that bike almost `[blank]` me!‚Äù

Can you guess what your friend said??

You‚Äôre naturally able to predict the missing word by considering the words bidirectionally before and after the missing word as context clues (in addition to your historical knowledge of how fishing works). Did you guess that your friend said, `hit`? That‚Äôs what we predicted as well but even we humans are error-prone to some of these methods. 
<img src="https://amitness.com/posts/images/bert-masked-language-model.png">

A random 15% of tokenized words are hidden during training and BERT‚Äôs job is to correctly predict the hidden words. Thus, directly teaching the model about the English language (and the words we use). Fun Fact: masking has been around a long time - <a href="https://psycnet.apa.org/record/1955-00850-001">1953 Paper on Cloze procedure</a> (or ‚ÄòMasking‚Äô).

## Next Sentence Prediction
NSP (Next Sentence Prediction) is used to help BERT learn about relationships between sentences by predicting if a given sentence follows the previous sentence or not. In training, 50% correct sentence pairs are mixed in with 50% random sentence pairs to help BERT increase next sentence prediction accuracy.
<img src="https://amitness.com/posts/images/bert-nsp.png">

Fun Fact: BERT is trained on both MLM (50%) and NSP (50%) at the same time.

# How to use BERT?
You can use this <a href="We've created this notebook so you can try BERT through this easy tutorial in Google Colab. ">notebook</a> from HuggingFace to start experimenting with BERT.
```python
!pip install transformers

from transformers import pipeline
unmasker = pipeline('fill-mask', model='bert-base-uncased')
unmasker("Artificial Intelligence [MASK] take over the world.")
```
When you run the above code you should see an output like this:
```
[{'score': 0.3182411789894104,
  'sequence': 'artificial intelligence can take over the world.',
  'token': 2064,
  'token_str': 'can'},
 {'score': 0.18299679458141327,
  'sequence': 'artificial intelligence will take over the world.',
  'token': 2097,
  'token_str': 'will'},
 {'score': 0.05600147321820259,
  'sequence': 'artificial intelligence to take over the world.',
  'token': 2000,
  'token_str': 'to'},
 {'score': 0.04519503191113472,
  'sequence': 'artificial intelligences take over the world.',
  'token': 2015,
  'token_str': '##s'},
 {'score': 0.045153118669986725,
  'sequence': 'artificial intelligence would take over the world.',
  'token': 2052,
  'token_str': 'would'}]
```
Not to worry, this is only BERT predicting the highest possible word to fill in the `[MASK]` token - or is it not?

# Problems with BERT
BERT, when released, yielded state of art results on many NLP tasks on leaderboards. But, the model was very large which resulted in some issues.

## Model Size
BERT-large, being a complex model, has 340 million parameters because of its 24 hidden layers and lots of nodes in the feed-forward network and attention heads. If you wanted to build upon the work on BERT and bring improvements to it, you would require large compute requirements to train from scratch and iterate on it.
<img src="https://amitness.com/posts/images/bert-heavy-on-gpu.png">

## Model Degradation
If larger models lead to better performance, why not double the hidden layer units of the largest available BERT model(BERT-large) from 1024 units to 2048 units? The paper <a href="https://arxiv.org/abs/1909.11942">ALBERT</a> dis just that. They call it ‚ÄúBERT-xlarge‚Äù. Surprisingly, **the larger model performs worse** than the BERT-large model on both the Language Modeling task as well as when tested on a reading comprehension test (RACE).
<img src="https://amitness.com/posts/images/bert-doubled-performance-race.png">

# Improvements for BERT

* **Cross-layer parameter sharing**: let's take the example of a 12-layer BERT-base model. Instead of learning unique parameters for each of the 12 layers, we only learn parameters for the first block and reuse the block in the remaining 11 layers.

* **Sentence Order Prediction**: the basic process of NSP in BERT training is:
    * Take two segments that appear consecutively from the training corpus
    * Create a random pair of segments from the different document as negative examples
    <img src="https://amitness.com/posts/images/nsp-training-data-generation.png">
    The ineffectiveness of NSP has made impact on the downstream tasks unreliable. ALBERT propose an alternative task called ‚ÄúSentence Order Prediction‚Äù. The key idea is:
    * Take two consecutive segments from the same document as a positive class
    * Swap the order of the same segment and use that as a negative example
    <img src="https://amitness.com/posts/images/sentence-order-prediction.png">

* **Factorized embedding parameterization**: a large matrix [30k x 768] can be factorized into 2 smaller matrices of size [30k x 100] and [100 x 768]. In this way, the total amount of parameters will significantly reduced. This is applied to the basic BERT, which has a vocabulary for 30,000 distinct tokens and each token can be embedded into a 768 dimension vector.
<img src="https://amitness.com/posts/images/embedding-decompose-albert.png">

ALBERT (A Lite BERT) applied the above techniques to produce a much lighter but still powerful BERT model.
* 18x fewer parameters than BERT-large
* Trained 1.7x faster
* Got SOTA results on GLUE, RACE and SQUAD during its release
    * RACE: 89.4% (+45.3% improvement)
    * GLUE Benchmark: 89.4
    * SQUAD 2.0 F1-score: 92.2

# Resource
* <a href="https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb">A Visual Notebook to Using BERT for the First Time</a>
* <a href="https://arxiv.org/abs/1907.11692">RoBERTa</a> paper
* <a href="https://www.youtube.com/watch?v=90mGPxR2GgY">BERT Explained</a> by Umar Jamil