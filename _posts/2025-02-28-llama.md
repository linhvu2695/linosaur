---
date: 2025-02-28 13:07:12
layout: post
title: LLaMA - Paper Read
subtitle: Attention techniques like MHA (Multi-Head), MQA (Multi-Query) & GQA (Grouped-Query). KV cache, SwiGLU and Rotary Positional Embedding
description: Attention techniques like MHA (Multi-Head), MQA (Multi-Query) & GQA (Grouped-Query). KV cache, SwiGLU and Rotary Positional Embedding
image: https://analyticsindiamag.com/wp-content/uploads/2023/07/Llama-2-is-Communist--768x432.jpg
optimized_image: https://analyticsindiamag.com/wp-content/uploads/2023/07/Llama-2-is-Communist--768x432.jpg
category: model
tags:
  - nlp
  - ai
  - model
author: linhvu2695
paginate: true
---
The Llama model architecture is built on the robust foundation of the transformer architecture, a neural network design that excels in natural language processing tasks. This architecture leverages a combination of self-attention mechanisms and feedforward neural networks to process sequences of text, making it highly effective for large-scale language modeling. In this article, we are going through several innovative elements that LLaMA used to boost efficiency:
* **Grouped Query Attention (GQA)**: GQA is a variant of the self-attention mechanism designed to be more computationally efficient, reducing the resources needed for processing.
* **SwiGLU Activation Function**: This activation function enhances the model’s efficiency, allowing it to perform complex computations with fewer resources.
* **Rotary Positional Embedding**: This type of positional embedding improves the model’s ability to handle long sequences of text, maintaining context over extended conversations.
<img src="https://images.hasgeek.com/embed/file/92cfd4fcc3474a5687607f82c6cfc184">

# Multi-Query Attention (MQA)
Multi-Query Attention was introduced in the paper <a href="https://arxiv.org/abs/1911.02150">Fast Transformer Decoding: One Write-Head is All You Need</a>.

**Review**: In standard multi-head attention (**MHA**), each attention head has its own set of query (Q), key (K), and value (V) projections. This means that for `H` heads, each token will have `H` query, key, and value embeddings.
<img src="https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/Screenshot_from_2023-06-16_13-08-52-thumbnail_webp-600x300.webp">

MQA modifies this by using a **single shared key and value head** across all query heads. There are still `H` query embeddings but only **one** key and value embedding. All of these embeddings are of size *d_model/H*. This significantly reduces the memory footprint of the model. This is especially impactful when decoding with a long context length. For example, with 32 attention heads, MQA requires only 1/32 of the KV cache memory compared to standard multi-head attention.
 <img src="https://rohitbandaru.github.io/assets/img/blog/transformer_pt2/mqa_gqa.png">

# Grouped-Query Attention (GQA)
Group-Query Attention was introduced in the paper <a href="https://arxiv.org/abs/2305.13245">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a>

GQA is a middle ground between standard multi-head attention and Multi-Query Attention (MQA). Instead of sharing a single key-value head across all query heads (as in MQA), GQA shares key-value heads among **groups of query heads**. For example, if we have 8 query heads and 2 key-value heads, each key-value head would be shared by 4 query heads. This provides a better balance between computational efficiency and model quality compared to MQA.

GQA is currently more popular as it offers a tunable tradeoff between efficiency and quality. MQA is a special case of GQA where the number of groups is 1.

# Key-Value Cache
The Key-Value (KV) Cache aimed to optimize the process when doing the Next Token Prediction task. During an iteration i-th, (1) the attention value of rpevious tokens does not change and (2) new tokens don’t influence the embeddings of previous tokens since it's causal.
<img src="https://res.cloudinary.com/dptj6j9y9/image/upload/v1740833635/Screenshot_2025-03-01_at_7.53.41_PM_n8svka.png">

Now for every iteration, we only need to compute a fraction of the huge attention matrix, significantly reduce computation effort.
<img src="https://res.cloudinary.com/dptj6j9y9/image/upload/v1740834199/Screenshot_2025-03-01_at_8.02.44_PM_j1rvbw.png">

KV caching makes a big difference in speed and efficiency, especially for long texts. By saving and reusing past calculations, it avoids the need to start over each time, making it much faster than the regular way of generating text.

# Rotary Positional Embedding (RoPE)
**Review**: Transformer model is different from RNN - they don't iterate through the sequence, but process the whole sequence in one round, thus struggling with the concept of position. **Positional Embedding** is composed to handle this challenge, and there are two main approaches:

1. **Absolute Position Embedding**: each position in the sequence is encoded with a vector. The encoding is usually based on sinusoidal functions
* Pros: easy to understand and implement
* Cons: challenging to grasp the relationship between tokens and difficult to generalize with sequences that are longer than the training samples.
<img src="https://res.cloudinary.com/dptj6j9y9/image/upload/v1740830340/bFPI9_vrri15.png">
<img src="https://machinelearningmastery.com/wp-content/uploads/2022/01/PE5.png">
*The positional encoding matrix for n=10,000, d=512, sequence length=100*

2. **Relative Position Embedding**: this approach focus on encoding the relative postion between tokens - by adding the encodings directly to the attention matrix
* Pros: more efficient in grasping the context and generalize
* Cons: more computationally complex, which leads to poor performance
<img src="https://images.viblo.asia/59774746-aa85-473d-bf6a-1bc4c711965a.png">

The idea of **Rotary Position Embedding** is to rotating the embedding vectors in a multidimensional space. This provides a more natural way to represent sequential information. Sinusoidal embeddings may not be effective over long distances or in complex sequences where the positional relationship is key to interpretation. RoPE’s rotation-based method maintains the relative positional information, making it particularly adept at handling such challenges.
<img src="https://images.viblo.asia/29acb396-2921-4575-b8a1-afb9c762f9c2.png">
<img src="https://res.cloudinary.com/dptj6j9y9/image/upload/v1740832169/Screenshot_2025-03-01_at_7.29.04_PM_h2r16s.png">
*Two tokens will have the same (rotated) vector difference even when their positions are shifted along the sequence*
RoPE adeptly captures relationships between tokens across lengthy sequences, a challenge for conventional positional embeddings.

# SwiGLU
SwiGLU builds on the <a href="https://arxiv.org/abs/1710.05941v1">Swish</a> activation function. This is also referred to as SiLU (Sigmoid Linear Unit) in the GeLU paper.
<img src="https://kikaben.com/swiglu-2020/index_files/figure-html/cell-3-output-1.png">

**Review**: while ReLU has proven itself, it’s not without drawbacks: (1) ReLU’s gradient is undefined at x=0, which can lead to optimization issues, (2) for negative values, ReLU outputs a constant zero, causing the **Dying ReLU** problem where certain neurons never activate, and (3) ReLU is strictly increasing (aka **monotonic**), which can sometimes limit the expressiveness of the network.

* **Swish**: the Swish activation function is a smoother, more expressive activation function compared to ReLU.
<img src="https://media.licdn.com/dms/image/v2/D4D12AQF9cXx2rlkAsw/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1738089160104?e=1746057600&v=beta&t=FkZb-yqKhrjE5IjjYWBtHfFiAABBkd7huuLngUfT87g">

* **GLU**: Gated-Linear Unit, introduce in <a href="https://arxiv.org/abs/1612.08083">2016</a>, is a gating mechanism that adds more control over which parts of the input get activated.

<img src="https://rohitbandaru.github.io/assets/img/blog/transformer_pt2/swiglu.png">
GLUs modify the feed-forward network by introducing a gating mechanism that controls information flow, allowing the network to selectively emphasize or suppress different parts of the input. This works by adding another linear transformation of the input 
 that acts as the gating function. The gating function performs element-wise multiplication with the output of the first feedforward layer and activation function. SwiGLU is a GLU that uses Swish as the activation function.
 <img src="https://media.licdn.com/dms/image/v2/D4D12AQEfL1vT8z4qCg/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1738089729167?e=1746057600&v=beta&t=r7KhO1dAbFRmkMMTNE-zNsprykRCimbU_ygKzQ_7ZoU">
 SwiGLU is used by many recent LLMs such as Mixtral/Mistral, LLaMA, Google's PaLM and Qwen.



