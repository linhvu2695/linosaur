---
date: 2025-03-01 10:21:22
layout: post
title: Mistral - Paper Read
subtitle: Attention techniques like Sliding Window and Longformer, Sparse Mixture of Experts (SMoE) and Rolling Buffer
description: Attention techniques like Sliding Window and Longformer, Sparse Mixture of Experts (SMoE) and Rolling Buffer
image: https://www.techzine.eu/wp-content/uploads/2024/09/Mistral-AI.jpg
optimized_image: https://www.techzine.eu/wp-content/uploads/2024/09/Mistral-AI.jpg
category: model
tags:
  - nlp
  - ai
  - model
author: linhvu2695
paginate: true
---
Mistral 7B is an LLM engineered for superior performance and efficiency. It leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. In this article, we will visit several concepts that Mistral is utilizing for its LLM model:
* **Sliding Window Attention (SWA)**: replace the full window attention with a sliding window that allows the model to work with longer context
* **Rolling Buffer Cache**: the application of SWA allows a way to use smaller cache without compromising model quality
* **Sparse Mixture of Experts (SMoE)**: instead of a cumbersome dense feed-forward network, several experts are employed to improve the performance
<img src="https://res.cloudinary.com/dptj6j9y9/image/upload/v1740889374/1_y0yf7AO0BEcdZEHdV9uUwQ_vrkyjv.png">

# Sliding Window Attention
<img src="https://res.cloudinary.com/dptj6j9y9/image/upload/v1741013405/1_GeTEDUrIRveqAfqdpTptGg_a8birh.webp">
The number of operations in vanilla attention in quadratic in the sequence length, and the memory increases linearly with the number of tokens. To alleviate this issue, we use SWA: each token can attend to at most W tokens from the previous layers (e.g., W = 3). Note that tokens outside the sliding window still influence next word prediction. 

By employing a window size of W = 4096, SWA theoretically achieves an attention span of approximately 131K tokens. In practice with a sequence length of 16K and W = 4096, SWA modifications in FlashAttention and xFormers result in a 2x speed enhancement compared to vanilla attention methods.

## Longformer
<img src="https://res.cloudinary.com/dptj6j9y9/image/upload/v1741014660/1_FM4cABExMDZ0-3KHUsviXQ_v6jlrg.png">
Longformer self attention employs self attention on both a “local” context and a “global” context. Most tokens only attend “locally” to each other in SWA pattern. Special tokens, such as [CLS] or task-specific tokens, enity tokens, key phrases or important markers can be selected for global attention This combination of close-up and big-picture attention helps the Longformer work efficiently with really long documents, like ones with thousands of words or even more.

# Rolling Buffer Cache
<img src="https://res.cloudinary.com/dptj6j9y9/image/upload/v1741016820/1_kIuTf0Rnp8fgTPZXFvy_vw_vtycc9.webp">
A Rolling Buffer Cache, employs a fixed attention span to limit cache size. The cache is of fixed size W, and it stores keys and values for timestep i at position i mod W in the cache. When i exceeds W, earlier values are overwritten, halting cache size growth. For instance, with W = 3, on a 32k-token sequence, cache memory usage is reduced by 8x without compromising model quality.

## Pre-fill and Chunking
<img src="https://res.cloudinary.com/dptj6j9y9/image/upload/v1741016987/1_7KpmhQiWP5G6UuS75yu9jw_cvfdxb.webp">
In sequence generation, tokens are predicted sequentially based on prior context. To optimize efficiency, a (k, v) cache is pre-filled with the known prompt. If the prompt is very long, it is chunked into smaller segments using a chosen window size. Each chunk is used to pre-fill the cache. This approach involves computing attention both within the cache and over the current chunk, thus aiding in more effective sequence generation.

# Sparse Mixture of Experts
## What is Mixture of Experts?
The roots of MoEs come from the 1991 paper <a href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf">Adaptive Mixture of Local Experts</a>. The idea, akin to ensemble methods, was to have a supervised procedure for a system composed of separate networks, each handling a different subset of the training cases. Each separate network, or expert, specializes in a different region of the input space. How is the expert chosen? A gating network determines the weights for each expert. During training, both the expert and the gating are trained.

## How does it work?
Instead of dense feed-forward network (FFN) layers. MoE layers have a certain number of **experts** (e.g. 8), where each expert is a neural network. In practice, the experts are FFNs, but they can also be more complex networks or even a MoE itself, leading to hierarchical MoEs!

A **gate network or router**, that determines which tokens are sent to which expert. For example, in the image below, the token “More” is sent to the second expert, and the token "Parameters” is sent to the first network. We can send a token to more than one expert, which defines Sparsity. How to route a token to an expert is one of the big decisions when working with MoEs - the router is composed of learned parameters and is pretrained at the same time as the rest of the network.
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/moe/01_moe_layer.png">

# Mistral results
* Mistral 7B surpasses Llama 2 13B across all metrics and outperforms Llama 1 34B on most benchmarks.
* In particular, Mistral 7B displays superior performance in code, mathematics, and reasoning benchmarks.
<img src="https://res.cloudinary.com/dptj6j9y9/image/upload/v1741019045/1_CWy3QhKWh-2H-TVBu1kULw_yofrzs.webp">