---
date: 2025-03-04 21:53:12
layout: post
title: "Bitcask - simple and beautiful"
subtitle: An old fashioned but well designed and a great start for those who want to build for themselves an actual key-value database.
description: An old fashioned but well designed and a great start for those who want to build for themselves an actual key-value database.
image: https://img.freepik.com/premium-photo/computer-hard-disk-drives-hdd-ssd-circuit-board-motherboard-background-close-up-with-red-blue-lighting_150455-2115.jpg
optimized_image: https://img.freepik.com/premium-photo/computer-hard-disk-drives-hdd-ssd-circuit-board-motherboard-background-close-up-with-red-blue-lighting_150455-2115.jpg
category: database
tags:
  - database
author: linhvu2695
paginate: true
---
<a href="https://docs.riak.com/riak/kv/2.2.3/setup/planning/backend/bitcask/index.html">Bitcask</a> is a high performance Key-Value database that was released in 2009. Although few companies are using Bitcask now (because there are better solutions), Bitcask still has a “beautiful” design and is worth exploring. The problem requirements for a KV database are quite simple - 1 database to store key-value data:

* **get**(key)
* **put**(key, value)
* **delete**(key)

However, it is not easy to achieve high performance for these 3 operations on hard drives (HDD). This comes from the operating mechanism of HDD.

# HDD
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEix8_5be7vD5Gln5mi1bFNo5eACe9ylNglSmFFuXigE_27niB03HIBGGVWKMqJ6ftKp50cP9-EsvHn1jwJ-Ghr6VtnJFM33nnlxXhfbBE__yH1x7FA_DHmINxcD45bVnxKv4x0uTpD3kt7o/s500/36_HDD_Asset_GIF.gif">
Data in HDD is stored on sectors of magnetic disks (platters). To read or write data to magnetic disks, two steps are required:
1. Move the reader to the correct track.
2. Rotate the magnetic disk so that the reading head scans over the correct sector to be read/written.
<img src="https://res.cloudinary.com/dptj6j9y9/image/upload/v1741187817/1_wdZ9qPgHt0WqX9o9DAqyyg_n1zfyt.webp">

## Random IO vs Sequential IO
Reading and writing data to HDD, basically, has 2 types:
* **Sequential IO**: data is read/written to adjacent bits of consecutive sectors on the magnetic disk.
* **Random IO**: data is read/written to random cells on randomly distributed sectors on the magnetic disk.
<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRhKT1M9aoj-nNduH8V0DI0nTSv6V9BGQr7Ig&s">

Due to the mechanism of moving the read head and rotating the disk when reading/writing data, Random IO on HDD is much slower than Sequential IO. Taking advantage of this feature, Bitcask has minimized Random IO by **storing key-value data on append-only log files**, meaning that writing data to Bitcask is done by only inserting data at the end of the file ~ sequential IO.

# Data files
Bitcasks Key-Value data is stored in append-only files, with one active file for writing, the remaining non-active files are read-only. **All write operations (put, delete) are performed by adding an entry to the end of the active file**. This makes the latency and throughput of Bitcask write operations extremely good.

When the active file size reaches a predetermined threshold, it is automatically converted to a non-active read-only file and a new active file is created to continue the writing process.
<img src="https://user-images.githubusercontent.com/4745789/87866701-78fdb800-c9a2-11ea-9c35-9a706ac96d97.png">
Each entry in the data file is described as shown above, including:
* CRC: value used to detect data errors during transmission or storage (similar to checksum), for example the server crashed during data writing.
* Timestamp: the time the entry was written to the file.
* Key Size: size of the key.
* Value Size: size of the value.
* Key: the value of the key.
* Value: value of value.

# KeyDir
Optimizing write speed by using append-only files in Bitcask unintentionally slows down the data reading process. To perform the get(key) operation, we have to iterate through each Bitcask file and find the entry (key, value) with the latest timestamp.

To overcome this problem, Bitcask applied a common principle in the database: **When reading is slow, consider using an index**.

KeyDir is a hash table in RAM that stores the locations of all keys in Bitcask.
`key -> { file_id, value_position, value_size }`
Thanks to that, to get(key), instead of having to sequentially browse all files, we just need to read the key's position from KeyDir in O(1) , and from there have enough data (file id, position to read, amount of data to read) to directly read the value from the data file.
<img src="https://user-images.githubusercontent.com/4745789/87866707-96cb1d00-c9a2-11ea-9730-fc7f8cb79b92.png">

# Compaction
The append-only data file mechanism will consume a lot of memory over time because we only insert new values ​​without deleting old values. To solve this problem, Bitcask uses compaction mechanism to periodically merge read-only data files. This mechanism is quite popular in NoSQL databases using LSM Tree like Cassandra and ScyllaDB.

The compaction process proceeds as follows:
* iterate over each entry in each non-active file. This is easy because the entries in the data file have a well-defined structure.
* Check if the current entry is valid and latest data by locking and querying the corresponding entry in KeyDir?
    * no: skip entry
    * yes: insert entry at the end of new file and update KeyDir
* Unlock entry trong KeyDir.
* Delete old data file

Compaction will be automatically triggered by Bitcask when the number of dead keys reaches a predetermined threshold (default 60%) or the size of dead keys reaches a predetermined threshold (default 512MB)

# Operations
So, we have explored the general operating mechanism of Bitcask, let's review the details of each task:

1. **get(key)**
* Find data file id and offset of key using KeyDir: `{ file_id,  value_position, value_size } = KeyDir.get(key)`
* read the value of the key using the above information
2. **put(key, value)**: These two steps must be done atomically to avoid race conditions:
* insert (key, value) and meta information at the end of active file
* update or create a new entry in KeyDir
3. **delete(key)**
The key deletion operation can be performed using put(key, tombstone) , where tombstone is a special value to represent the keys to be deleted.

# Pros and Cons of Bitcask
## Advantage
* Low latency and stable.
* High write throughput, ~5000-6000 qps.
* Easy to backup and restore because data files are mostly read-only.
* Simple and easy to understand design

## Disadvantages
* Store all keys in KeyDir in RAM, which is a major limitation for large systems.
* Does not support range queries, for example get all keys in the range [min, max).
* No advanced features like transactions, complex data types and queries.

## Lesson learned
* On HDDs, sequential IO performance is significantly superior to random IO. Writing data to append-only files is also faster.
* When reading is slow, consider using an index.
* When storing data, there should be a checksum/CRC to ensure integrity.